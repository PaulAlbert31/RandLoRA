# Parameter-Efficient Fine-tuning on GLUE

This repository provides scripts for fine-tuning pre-trained language models on the GLUE benchmark using parameter-efficient techniques like LoRA, RandLoRA, and VeRA. It also includes a script to parse and aggregate the evaluation results.
This code is based of Huggingface's GLUE [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).

## Scripts

### `run_glue.py`

This script is based on the Hugging Face Transformers library's `run_glue.py` example and has been extended to incorporate LoRA, RandLoRA, and VeRA for parameter-efficient fine-tuning.

**Key Features:**

- Supports fine-tuning on various GLUE tasks.
- Implements LoRA, RandLoRA, and VeRA adapters.
- Allows customization of adapter rank and other hyperparameters.
- Utilizes the Hugging Face `Trainer` for efficient training and evaluation.

**Command-line Arguments:**

The script takes various command-line arguments to configure the model, dataset, training process, and adapter settings. Some important arguments include:

- `--model_name_or_path`: Path to the pre-trained model (e.g., `FacebookAI/roberta-base`).
- `--task_name`: The GLUE task to train on (e.g., `sst2`, `mrpc`).
- `--do_train`: Whether to perform training.
- `--do_eval`: Whether to perform evaluation.
- `--max_seq_length`: Maximum sequence length for input.
- `--per_device_train_batch_size`: Batch size per GPU/TPU core during training.
- `--learning_rate`: The initial learning rate for AdamW.
- `--num_train_epochs`: Total number of training epochs to perform.
- `--output_dir`: The output directory where the model predictions and checkpoints will be written.
- `--fp16`: Whether to use 16-bit (mixed) precision training.
- `--adapter_name`: The adapter to use (`lora`, `randlora`, or `vera`).
- `--rank`: The rank of the adapter (for LoRA, RandLoRA, and VeRA).
- `--seed`: Random seed for initialization.

### `parse_results.py`

This script parses the `eval_results.json` files generated by `run_glue.py` and calculates the average and standard deviation of the evaluation metrics across multiple runs (seeds).

**Usage:**

```bash
python parse_results.py <root_directory> <number_of_seeds>
```

- `<root_directory>`: The root directory containing the subdirectories for each seed and task (e.g., `finetuned_results/randlora64`).
- `<number_of_seeds>`: The number of different random seeds used in the experiments.

**Output:**

The script prints the average and standard deviation of the evaluation metrics for each task and the overall average across all tasks.

## Example Usage

The `train_all.sh` file provides an example of how to run the `run_glue.py` script for different models, tasks, and adapter configurations.

To replicate the example runs, you can execute the commands in `train_all.sh`. For instance, the following command fine-tunes `FacebookAI/roberta-base` on the `sst2` task using RandLoRA with a rank of 64:

```bash
python run_glue.py \
       --model_name_or_path FacebookAI/roberta-base \
       --task_name sst2 \
       --do_train \
       --do_eval \
       --max_seq_length 128 \
       --per_device_train_batch_size 64 \
       --learning_rate 1e-4 \
       --num_train_epochs 10 \
       --output_dir finetuned_results/randlora64/1/sst2/ \
       --fp16 \
       --adapter_name randlora \
       --rank 64 \
       --seed 1
```

After running the fine-tuning scripts for multiple seeds, you can use `parse_results.py` to analyze the results. For example, with the output directories organized like `finetuned_results/randlora64/1/sst2/`, `finetuned_results/randlora64/2/sst2/`, etc., and you ran for 5 seeds, you can run:

```bash
python parse_results.py finetuned_results/randlora64 5
```

This will output the aggregated evaluation results for the experiments in the `finetuned_results/randlora64` directory.

## Dependencies

A conda env is provided in env.yml

```bash
conda env create -f env.yml
conda activate randlora_glue
```